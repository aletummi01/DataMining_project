
2025-11-11 15:33:48.182979: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-11 15:33:48.859381: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

Device: CUDA
Tokenizzazione del dataset completo (DistilBERT)...
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44898/44898 [02:47<00:00, 268.79 examples/s]

Eseguo Nested Cross-Validation (2x2)...

--- Outer Fold 1/2 ---
Inizio Tuning Iperparametri
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.3692, 'grad_norm': 0.7188565135002136, 'learning_rate': 4.76258309591643e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0364, 'grad_norm': 0.19114696979522705, 'learning_rate': 4.525166191832859e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0203, 'grad_norm': 0.11261282116174698, 'learning_rate': 4.287749287749288e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0132, 'grad_norm': 0.05406912416219711, 'learning_rate': 4.050332383665717e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0105, 'grad_norm': 0.044250208884477615, 'learning_rate': 3.8129154795821466e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0235, 'grad_norm': 2.731863021850586, 'learning_rate': 3.5754985754985762e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0141, 'grad_norm': 0.048200856894254684, 'learning_rate': 3.338081671415005e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.998218262806236, 'eval_precision': 0.999064371257485, 'eval_recall': 0.9971983563690698, 'eval_f1': 0.998130491680688, 'eval_loss': 0.009672031737864017, 'eval_runtime': 31.5451, 'eval_samples_per_second': 355.84, 'eval_steps_per_second': 22.254, 'epoch': 1.0}
{'loss': 0.0126, 'grad_norm': 0.028276344761252403, 'learning_rate': 3.1006647673314343e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0126, 'grad_norm': 0.03874065726995468, 'learning_rate': 2.8632478632478635e-06, 'epoch': 1.282051282051282}
{'loss': 0.0083, 'grad_norm': 0.02424537017941475, 'learning_rate': 2.6258309591642927e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0177, 'grad_norm': 0.02475924976170063, 'learning_rate': 2.388414055080722e-06, 'epoch': 1.566951566951567}
{'loss': 0.007, 'grad_norm': 0.1921364665031433, 'learning_rate': 2.150997150997151e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0049, 'grad_norm': 0.017846835777163506, 'learning_rate': 1.9135802469135804e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0036, 'grad_norm': 0.05460571497678757, 'learning_rate': 1.6761633428300099e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9984855233853007, 'eval_precision': 0.9994385176866929, 'eval_recall': 0.9973851326111318, 'eval_f1': 0.9984107693745911, 'eval_loss': 0.008191585540771484, 'eval_runtime': 31.9403, 'eval_samples_per_second': 351.437, 'eval_steps_per_second': 21.979, 'epoch': 2.0}
{'loss': 0.0124, 'grad_norm': 0.015094599686563015, 'learning_rate': 1.4387464387464389e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0112, 'grad_norm': 0.015618490986526012, 'learning_rate': 1.2013295346628681e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0039, 'grad_norm': 0.012877039611339569, 'learning_rate': 9.639126305792973e-07, 'epoch': 2.421652421652422}
{'loss': 0.0067, 'grad_norm': 0.014068310149013996, 'learning_rate': 7.264957264957266e-07, 'epoch': 2.564102564102564}
{'loss': 0.001, 'grad_norm': 0.013646770268678665, 'learning_rate': 4.890788224121558e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0019, 'grad_norm': 0.012749891728162766, 'learning_rate': 2.51661918328585e-07, 'epoch': 2.849002849002849}
{'loss': 0.0041, 'grad_norm': 0.011707809753715992, 'learning_rate': 1.4245014245014247e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9983073496659243, 'eval_precision': 0.9990645463049579, 'eval_recall': 0.9973851326111318, 'eval_f1': 0.9982241330965511, 'eval_loss': 0.007496611215174198, 'eval_runtime': 32.2764, 'eval_samples_per_second': 347.777, 'eval_steps_per_second': 21.75, 'epoch': 3.0}
{'train_runtime': 411.744, 'train_samples_per_second': 81.779, 'train_steps_per_second': 5.115, 'train_loss': 0.028268097270128, 'epoch': 3.0}
{'eval_accuracy': 0.9983073496659243, 'eval_precision': 0.9990645463049579, 'eval_recall': 0.9973851326111318, 'eval_f1': 0.9982241330965511, 'eval_loss': 0.007496611215174198, 'eval_runtime': 31.6025, 'eval_samples_per_second': 355.194, 'eval_steps_per_second': 22.213, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.4447, 'grad_norm': 1.1745723485946655, 'learning_rate': 4.76258309591643e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0469, 'grad_norm': 0.2392149567604065, 'learning_rate': 4.525166191832859e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0115, 'grad_norm': 0.11502817273139954, 'learning_rate': 4.287749287749288e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0139, 'grad_norm': 0.07239248603582382, 'learning_rate': 4.050332383665717e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0108, 'grad_norm': 0.05348611995577812, 'learning_rate': 3.8129154795821466e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0082, 'grad_norm': 0.04933157190680504, 'learning_rate': 3.5754985754985762e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0156, 'grad_norm': 0.030116287991404533, 'learning_rate': 3.338081671415005e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9971489665003563, 'eval_precision': 0.9975691847419597, 'eval_recall': 0.9964512514008218, 'eval_f1': 0.997009904690712, 'eval_loss': 0.0124 0.012460445985198021, 'eval_runtime': 31.3647, 'eval_samples_per_second': 357.854, 'eval_steps_per_second': 22.382, 'epoch': 1.0}
{'loss': 0.0038, 'grad_norm': 0.026149999350309372, 'learning_rate': 3.1006647673314343e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0095, 'grad_norm': 0.02865862473845482, 'learning_rate': 2.8632478632478635e-06, 'epoch': 1.282051282051282}
{'loss': 0.0087, 'grad_norm': 0.022463668137788773, 'learning_rate': 2.6258309591642927e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0084, 'grad_norm': 0.0324324369430542, 'learning_rate': 2.388414055080722e-06, 'epoch': 1.566951566951567}
{'loss': 0.0013, 'grad_norm': 0.017905622720718384, 'learning_rate': 2.150997150997151e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0118, 'grad_norm': 0.02155366726219654, 'learning_rate': 1.9135802469135804e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0038, 'grad_norm': 0.018435686826705933, 'learning_rate': 1.6761633428300099e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9981290092658589, 'eval_precision': 0.9985043933445504, 'eval_recall': 0.9975719088531939, 'eval_f1': 0.9980379332897319, 'eval_loss': 0.008645644411444664, 'eval_runtime': 32.9431, 'eval_samples_per_second': 340.708, 'eval_steps_per_second': 21.309, 'epoch': 2.0}
{'loss': 0.0041, 'grad_norm': 0.01854533515870571, 'learning_rate': 1.4387464387464389e-06, 'epoch': 2.1367521367521367}
{'loss': 0.001, 'grad_norm': 0.016448698937892914, 'learning_rate': 1.2013295346628681e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0011, 'grad_norm': 0.013842550106346607, 'learning_rate': 9.639126305792973e-07, 'epoch': 2.421652421652422}
{'loss': 0.0008, 'grad_norm': 0.015122471377253532, 'learning_rate': 7.264957264957266e-07, 'epoch': 2.564102564102564}
{'loss': 0.0029, 'grad_norm': 0.015065925195813179, 'learning_rate': 4.890788224121558e-07, 'epoch': 2.7065527065527064}
{'loss': 0.004, 'grad_norm': 0.012453686445951462, 'learning_rate': 2.51661918328585e-07, 'epoch': 2.849002849002849}
{'loss': 0.0099, 'grad_norm': 0.01517232321202755, 'learning_rate': 1.4245014245014247e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9982181040627227, 'eval_precision': 0.9985046728971962, 'eval_recall': 0.9977586850952559, 'eval_f1': 0.9981315396113603, 'eval_loss': 0.00857030600309372, 'eval_runtime': 32.7163, 'eval_samples_per_second': 343.07, 'eval_steps_per_second': 21.457, 'epoch': 3.0}
{'train_runtime': 416.1297, 'train_samples_per_second': 80.924, 'train_steps_per_second': 5.061, 'train_loss': 0.029568489409226073, 'epoch': 3.0}
{'eval_accuracy': 0.9982181040627227, 'eval_precision': 0.9985046728971962, 'eval_recall': 0.9977586850952559, 'eval_f1': 0.9981315396113603, 'eval_loss': 0.00857030600309372, 'eval_runtime': 32.4502, 'eval_samples_per_second': 345.884, 'eval_steps_per_second': 21.633, 'epoch': 3.0}
LR 5.0e-06 - Media Inner Acc: 0.9983
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2652, 'grad_norm': 0.23773843050003052, 'learning_rate': 9.52516619183286e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0186, 'grad_norm': 0.07003401964902878, 'learning_rate': 9.050332383665718e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0144, 'grad_norm': 0.046337027102708817, 'learning_rate': 8.575498575498576e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0075, 'grad_norm': 0.030230781063437462, 'learning_rate': 8.100664767331435e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0087, 'grad_norm': 0.023043233901262283, 'learning_rate': 7.625830959164293e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0111, 'grad_norm': 11.343996047973633, 'learning_rate': 7.1509971509971524e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0118, 'grad_norm': 0.019938481971621513, 'learning_rate': 6.67616334283001e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9990200445434299, 'eval_precision': 0.999252476172678, 'eval_recall': 0.9986925663055659, 'eval_f1': 0.9989724427837459, 'eval_loss': 0.005143192131072283, 'eval_runtime': 32.5551, 'eval_samples_per_second': 344.8, 'eval_steps_per_second': 21.563, 'epoch': 1.0}
{'loss': 0.0044, 'grad_norm': 0.012892082333564758, 'learning_rate': 6.2013295346628685e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0102, 'grad_norm': 0.019065048545598984, 'learning_rate': 5.726495726495727e-06, 'epoch': 1.282051282051282}
{'loss': 0.0007, 'grad_norm': 0.00942299235612154, 'learning_rate': 5.2516619183285854e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0108, 'grad_norm': 0.00883042998611927, 'learning_rate': 4.776828110161444e-06, 'epoch': 1.566951566951567}
{'loss': 0.0008, 'grad_norm': 0.6675756573677063, 'learning_rate': 4.301994301994302e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0051, 'grad_norm': 0.009684906341135502, 'learning_rate': 3.827160493827161e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0026, 'grad_norm': 0.01966652274131775, 'learning_rate': 3.3523266856600197e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.9990662931839402, 'eval_recall': 0.999252895031752, 'eval_f1': 0.9991595853954618, 'eval_loss': 0.004940073937177658, 'eval_runtime': 32.5795, 'eval_samples_per_second': 344.542, 'eval_steps_per_second': 21.547, 'epoch': 2.0}
{'loss': 0.0005, 'grad_norm': 0.00598612055182457, 'learning_rate': 2.8774928774928778e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0042, 'grad_norm': 0.006556440610438585, 'learning_rate': 2.4026590693257362e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0018, 'grad_norm': 0.005430007819086313, 'learning_rate': 1.9278252611585947e-06, 'epoch': 2.421652421652422}
{'loss': 0.0074, 'grad_norm': 0.005417325999587774, 'learning_rate': 1.4529914529914531e-06, 'epoch': 2.564102564102564}
{'loss': 0.0003, 'grad_norm': 0.005937311798334122, 'learning_rate': 9.781576448243116e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0008, 'grad_norm': 0.00613733846694231, 'learning_rate': 5.0332383665717e-07, 'epoch': 2.849002849002849}
{'loss': 0.0003, 'grad_norm': 0.004362627863883972, 'learning_rate': 2.8490028490028493e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9990200445434299, 'eval_precision': 0.9986932984879596, 'eval_recall': 0.999252895031752, 'eval_f1': 0.998973018392307, 'eval_loss': 0.00453304685652256, 'eval_runtime': 32.4812, 'eval_samples_per_second': 345.584, 'eval_steps_per_second': 21.612, 'epoch': 3.0}
{'train_runtime': 419.8747, 'train_samples_per_second': 80.195, 'train_steps_per_second': 5.016, 'train_loss': 0.01838400874166163, 'epoch': 3.0}
{'eval_accuracy': 0.9990200445434299, 'eval_precision': 0.9986932984879596, 'eval_recall': 0.999252895031752, 'eval_f1': 0.998973018392307, 'eval_loss': 0.00453304685652256, 'eval_runtime': 31.9043, 'eval_samples_per_second': 351.834, 'eval_steps_per_second': 22.003, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.272, 'grad_norm': 0.27831849455833435, 'learning_rate': 9.52516619183286e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0161, 'grad_norm': 0.08883137255907059, 'learning_rate': 9.050332383665718e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0037, 'grad_norm': 0.03897099196910858, 'learning_rate': 8.575498575498576e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0106, 'grad_norm': 0.028438707813620567, 'learning_rate': 8.100664767331435e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0089, 'grad_norm': 0.021612655371427536, 'learning_rate': 7.625830959164293e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0028, 'grad_norm': 0.01857266202569008, 'learning_rate': 7.1509971509971524e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0161, 'grad_norm': 0.045500412583351135, 'learning_rate': 6.67616334283001e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9979508196721312, 'eval_precision': 0.9985038339255657, 'eval_recall': 0.9971983563690698, 'eval_f1': 0.9978506681618541, 'eval_loss': 0.009674509055912495, 'eval_runtime': 31.5199, 'eval_samples_per_second': 356.092, 'eval_steps_per_second': 22.272, 'epoch': 1.0}
{'loss': 0.0023, 'grad_norm': 0.011150943115353584, 'learning_rate': 6.2013295346628685e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0039, 'grad_norm': 0.01209814939647913, 'learning_rate': 5.726495726495727e-06, 'epoch': 1.282051282051282}
{'loss': 0.0082, 'grad_norm': 0.009795840829610825, 'learning_rate': 5.2516619183285854e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0019, 'grad_norm': 0.012502023950219154, 'learning_rate': 4.776828110161444e-06, 'epoch': 1.566951566951567}
{'loss': 0.0005, 'grad_norm': 0.007710888981819153, 'learning_rate': 4.301994301994302e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0019, 'grad_norm': 0.00975646823644638, 'learning_rate': 3.827160493827161e-06, 'epoch': 1.8518518518518519}
{'loss': 0.002, 'grad_norm': 0.00744791841134429, 'learning_rate': 3.3523266856600197e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9987526728439059, 'eval_precision': 0.999438832772166, 'eval_recall': 0.9979454613373179, 'eval_f1': 0.9986915887850467, 'eval_loss': 0.007269896566867828, 'eval_runtime': 31.9016, 'eval_samples_per_second': 351.831, 'eval_steps_per_second': 22.005, 'epoch': 2.0}
{'loss': 0.0012, 'grad_norm': 0.009726936928927898, 'learning_rate': 2.8774928774928778e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0003, 'grad_norm': 0.006383529398590326, 'learning_rate': 2.4026590693257362e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0003, 'grad_norm': 0.005225014872848988, 'learning_rate': 1.9278252611585947e-06, 'epoch': 2.421652421652422}
{'loss': 0.0003, 'grad_norm': 0.006145216524600983, 'learning_rate': 1.4529914529914531e-06, 'epoch': 2.564102564102564}
{'loss': 0.0007, 'grad_norm': 0.005810936447232962, 'learning_rate': 9.781576448243116e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0003, 'grad_norm': 0.004826457239687443, 'learning_rate': 5.0332383665717e-07, 'epoch': 2.849002849002849}
{'loss': 0.0038, 'grad_norm': 0.005678960122168064, 'learning_rate': 2.8490028490028493e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9988417676407698, 'eval_precision': 0.9992521966722752, 'eval_recall': 0.998319013821442, 'eval_f1': 0.9987853872745959, 'eval_loss': 0.007678041234612465, 'eval_runtime': 31.4681, 'eval_samples_per_second': 356.679, 'eval_steps_per_second': 22.308, 'epoch': 3.0}
{'train_runtime': 412.6446, 'train_samples_per_second': 81.608, 'train_steps_per_second': 5.104, 'train_loss': 0.016994792610386535, 'epoch': 3.0}
{'eval_accuracy': 0.9987526728439059, 'eval_precision': 0.999438832772166, 'eval_recall': 0.9979454613373179, 'eval_f1': 0.9986915887850467, 'eval_loss': 0.007269896566867828, 'eval_runtime': 31.6425, 'eval_samples_per_second': 354.712, 'eval_steps_per_second': 22.185, 'epoch': 3.0}
LR 1.0e-05 - Media Inner Acc: 0.9989
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.1929, 'grad_norm': 0.10398721694946289, 'learning_rate': 1.4287749287749289e-05, 'epoch': 0.14245014245014245}
{'loss': 0.0147, 'grad_norm': 0.04174568131566048, 'learning_rate': 1.3575498575498576e-05, 'epoch': 0.2849002849002849}
{'loss': 0.0145, 'grad_norm': 0.031386349350214005, 'learning_rate': 1.2863247863247864e-05, 'epoch': 0.42735042735042733}
{'loss': 0.0058, 'grad_norm': 0.02002706378698349, 'learning_rate': 1.2150997150997151e-05, 'epoch': 0.5698005698005698}
{'loss': 0.0085, 'grad_norm': 0.014921074733138084, 'learning_rate': 1.143874643874644e-05, 'epoch': 0.7122507122507122}
{'loss': 0.0062, 'grad_norm': 8.59961223602295, 'learning_rate': 1.0726495726495728e-05, 'epoch': 0.8547008547008547}
{'loss': 0.0127, 'grad_norm': 0.016592374071478844, 'learning_rate': 1.0014245014245015e-05, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.9994393571295085, 'eval_recall': 0.9988793425476279, 'eval_f1': 0.9991592713685193, 'eval_loss': 0.004771565552800894, 'eval_runtime': 31.5045, 'eval_samples_per_second': 356.298, 'eval_steps_per_second': 22.283, 'epoch': 1.0}
{'loss': 0.0061, 'grad_norm': 0.007735807914286852, 'learning_rate': 9.301994301994302e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0086, 'grad_norm': 0.01297245267778635, 'learning_rate': 8.589743589743589e-06, 'epoch': 1.282051282051282}
{'loss': 0.0004, 'grad_norm': 0.005627704318612814, 'learning_rate': 7.877492877492877e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0103, 'grad_norm': 0.006102162878960371, 'learning_rate': 7.165242165242165e-06, 'epoch': 1.566951566951567}
{'loss': 0.0005, 'grad_norm': 0.2925981283187866, 'learning_rate': 6.452991452991453e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0052, 'grad_norm': 0.0074185640551149845, 'learning_rate': 5.74074074074074e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0014, 'grad_norm': 0.005789899732917547, 'learning_rate': 5.028490028490029e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9993763919821826, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.999252895031752, 'eval_f1': 0.9993462220976931, 'eval_loss': 0.004198761191219091, 'eval_runtime': 31.4133, 'eval_samples_per_second': 357.332, 'eval_steps_per_second': 22.347, 'epoch': 2.0}
{'loss': 0.0003, 'grad_norm': 0.0034488793462514877, 'learning_rate': 4.316239316239317e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0034, 'grad_norm': 0.0036610232200473547, 'learning_rate': 3.6039886039886043e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0004, 'grad_norm': 0.0036803099792450666, 'learning_rate': 2.8917378917378916e-06, 'epoch': 2.421652421652422}
{'loss': 0.0065, 'grad_norm': 0.003895939327776432, 'learning_rate': 2.1794871794871797e-06, 'epoch': 2.564102564102564}
{'loss': 0.0002, 'grad_norm': 0.004399471916258335, 'learning_rate': 1.4672364672364672e-06, 'epoch': 2.7065527065527064}
{'loss': 0.0003, 'grad_norm': 0.0046529704704880714, 'learning_rate': 7.54985754985755e-07, 'epoch': 2.849002849002849}
{'loss': 0.0002, 'grad_norm': 0.002706771017983556, 'learning_rate': 4.2735042735042736e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9992873051224944, 'eval_precision': 0.999252895031752, 'eval_recall': 0.999252895031752, 'eval_f1': 0.999252895031752, 'eval_loss': 0.004259902983903885, 'eval_runtime': 32.0508, 'eval_samples_per_second': 350.225, 'eval_steps_per_second': 21.903, 'epoch': 3.0}
{'train_runtime': 408.156, 'train_samples_per_second': 82.498, 'train_steps_per_second': 5.16, 'train_loss': 0.014207751119537199, 'epoch': 3.0}
{'eval_accuracy': 0.9993763919821826, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.999252895031752, 'eval_f1': 0.9993462220976931, 'eval_loss': 0.004198761191219091, 'eval_runtime': 31.8682, 'eval_samples_per_second': 352.232, 'eval_steps_per_second': 22.028, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2045, 'grad_norm': 0.1478087306022644, 'learning_rate': 1.4287749287749289e-05, 'epoch': 0.14245014245014245}
{'loss': 0.0101, 'grad_norm': 0.07432019710540771, 'learning_rate': 1.3575498575498576e-05, 'epoch': 0.2849002849002849}
{'loss': 0.002, 'grad_norm': 0.020526014268398285, 'learning_rate': 1.2863247863247864e-05, 'epoch': 0.42735042735042733}
{'loss': 0.0088, 'grad_norm': 0.016364816576242447, 'learning_rate': 1.2150997150997151e-05, 'epoch': 0.5698005698005698}
{'loss': 0.006, 'grad_norm': 0.012668490409851074, 'learning_rate': 1.143874643874644e-05, 'epoch': 0.7122507122507122}
{'loss': 0.0007, 'grad_norm': 0.0103239631280303, 'learning_rate': 1.0726495726495728e-05, 'epoch': 0.8547008547008547}
{'loss': 0.0172, 'grad_norm': 0.08285900205373764, 'learning_rate': 1.0014245014245015e-05, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9986635780470421, 'eval_precision': 0.9996256784577953, 'eval_recall': 0.9975719088531939, 'eval_f1': 0.9985977376834626, 'eval_loss': 0.007229068782180548, 'eval_runtime': 32.162, 'eval_samples_per_second': 348.984, 'eval_steps_per_second': 21.827, 'epoch': 1.0}
{'loss': 0.0019, 'grad_norm': 0.0067779505625367165, 'learning_rate': 9.301994301994302e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0019, 'grad_norm': 0.0071427784860134125, 'learning_rate': 8.589743589743589e-06, 'epoch': 1.282051282051282}
{'loss': 0.0052, 'grad_norm': 0.008717024698853493, 'learning_rate': 7.877492877492877e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0056, 'grad_norm': 0.011227427050471306, 'learning_rate': 7.165242165242165e-06, 'epoch': 1.566951566951567}
{'loss': 0.0004, 'grad_norm': 0.006340174935758114, 'learning_rate': 6.452991452991453e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0004, 'grad_norm': 0.005165947135537863, 'learning_rate': 5.74074074074074e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0012, 'grad_norm': 0.004505835939198732, 'learning_rate': 5.028490028490029e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9988417676407698, 'eval_precision': 0.9992521966722752, 'eval_recall': 0.998319013821442, 'eval_f1': 0.9987853872745959, 'eval_loss': 0.0067874290980398655, 'eval_runtime': 32.244, 'eval_samples_per_second': 348.096, 'eval_steps_per_second': 21.771, 'epoch': 2.0}
{'loss': 0.0009, 'grad_norm': 0.006072173826396465, 'learning_rate': 4.316239316239317e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0002, 'grad_norm': 0.003729375312104821, 'learning_rate': 3.6039886039886043e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0002, 'grad_norm': 0.0032116754446178675, 'learning_rate': 2.8917378917378916e-06, 'epoch': 2.421652421652422}
{'loss': 0.0002, 'grad_norm': 0.0036925363820046186, 'learning_rate': 2.1794871794871797e-06, 'epoch': 2.564102564102564}
{'loss': 0.0004, 'grad_norm': 0.003361174836754799, 'learning_rate': 1.4672364672364672e-06, 'epoch': 2.7065527065527064}
{'loss': 0.0002, 'grad_norm': 0.0026613762602210045, 'learning_rate': 7.54985754985755e-07, 'epoch': 2.849002849002849}
{'loss': 0.0041, 'grad_norm': 0.0034693304914981127, 'learning_rate': 4.2735042735042736e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9988417676407698, 'eval_precision': 0.9992521966722752, 'eval_recall': 0.998319013821442, 'eval_f1': 0.9987853872745959, 'eval_loss': 0.007250117138028145, 'eval_runtime': 31.1869, 'eval_samples_per_second': 359.894, 'eval_steps_per_second': 22.509, 'epoch': 3.0}
{'train_runtime': 416.1046, 'train_samples_per_second': 80.929, 'train_steps_per_second': 5.061, 'train_loss': 0.012906117594167625, 'epoch': 3.0}
{'eval_accuracy': 0.9988417676407698, 'eval_precision': 0.9992521966722752, 'eval_recall': 0.998319013821442, 'eval_f1': 0.9987853872745959, 'eval_loss': 0.0067874290980398655, 'eval_runtime': 31.9113, 'eval_samples_per_second': 351.725, 'eval_steps_per_second': 21.999, 'epoch': 3.0}
LR 1.5e-05 - Media Inner Acc: 0.9991
Miglior LR per Fold 1: 1.5e-05 (Acc: 0.9991)
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.049, 'grad_norm': 0.057836730033159256, 'learning_rate': 1.321937321937322e-05, 'epoch': 0.3561253561253561}
{'loss': 0.0061, 'grad_norm': 0.005674898158758879, 'learning_rate': 1.143874643874644e-05, 'epoch': 0.7122507122507122}
{'loss': 0.0045, 'grad_norm': 0.01289698202162981, 'learning_rate': 9.658119658119659e-06, 'epoch': 1.0683760683760684}
{'loss': 0.0032, 'grad_norm': 0.018316561356186867, 'learning_rate': 7.877492877492877e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0015, 'grad_norm': 0.001454983721487224, 'learning_rate': 6.096866096866097e-06, 'epoch': 1.7806267806267806}
{'loss': 0.0019, 'grad_norm': 0.0013467567041516304, 'learning_rate': 4.316239316239317e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0004, 'grad_norm': 0.0009180995402857661, 'learning_rate': 2.5356125356125354e-06, 'epoch': 2.492877492877493}
{'loss': 0.0003, 'grad_norm': 0.01904376968741417, 'learning_rate': 7.54985754985755e-07, 'epoch': 2.849002849002849}
{'train_runtime': 1384.3743, 'train_samples_per_second': 48.648, 'train_steps_per_second': 3.043, 'train_loss': 0.00796650474292296, 'epoch': 3.0}
Accuracy TEST Fold 1: 0.9997

--- Outer Fold 2/2 ---
Inizio Tuning Iperparametri
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.4431, 'grad_norm': 1.526538372039795, 'learning_rate': 4.76258309591643e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0423, 'grad_norm': 0.1846856325864792, 'learning_rate': 4.525166191832859e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0186, 'grad_norm': 0.11328129470348358, 'learning_rate': 4.287749287749288e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0108, 'grad_norm': 0.07662826776504517, 'learning_rate': 4.050332383665717e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0051, 'grad_norm': 0.04651853069663048, 'learning_rate': 3.8129154795821466e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0057, 'grad_norm': 0.03555462881922722, 'learning_rate': 3.5754985754985762e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0038, 'grad_norm': 0.03632320836186409, 'learning_rate': 3.338081671415005e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9990200445434299, 'eval_precision': 0.9994392523364486, 'eval_recall': 0.9985060690943044, 'eval_f1': 0.9989724427837459, 'eval_loss': 0.005410592537373304, 'eval_runtime': 32.364, 'eval_samples_per_second': 346.836, 'eval_steps_per_second': 21.691, 'epoch': 1.0}
{'loss': 0.0053, 'grad_norm': 0.02508457936346531, 'learning_rate': 3.1006647673314343e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0077, 'grad_norm': 0.02206289954483509, 'learning_rate': 2.8632478632478635e-06, 'epoch': 1.282051282051282}
{'loss': 0.0038, 'grad_norm': 0.021404730156064034, 'learning_rate': 2.6258309591642927e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0012, 'grad_norm': 0.020609484985470772, 'learning_rate': 2.388414055080722e-06, 'epoch': 1.566951566951567}
{'loss': 0.001, 'grad_norm': 0.017426688224077225, 'learning_rate': 2.150997150997151e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0032, 'grad_norm': 0.017824407666921616, 'learning_rate': 1.9135802469135804e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0054, 'grad_norm': 0.015487462282180786, 'learning_rate': 1.6761633428300099e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9992873051224944, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.999252895031752, 'eval_loss': 0.0043438514694571495, 'eval_runtime': 33.8843, 'eval_samples_per_second': 331.274, 'eval_steps_per_second': 20.718, 'epoch': 2.0}
{'loss': 0.0008, 'grad_norm': 0.012529074214398861, 'learning_rate': 1.4387464387464389e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0083, 'grad_norm': 0.014570387080311775, 'learning_rate': 1.2013295346628681e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0007, 'grad_norm': 0.012021677568554878, 'learning_rate': 9.639126305792973e-07, 'epoch': 2.421652421652422}
{'loss': 0.0007, 'grad_norm': 0.011109831742942333, 'learning_rate': 7.264957264957266e-07, 'epoch': 2.564102564102564}
{'loss': 0.0012, 'grad_norm': 0.012960554100573063, 'learning_rate': 4.890788224121558e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0054, 'grad_norm': 0.010431285947561264, 'learning_rate': 2.51661918328585e-07, 'epoch': 2.849002849002849}
{'loss': 0.0006, 'grad_norm': 0.010574137791991234, 'learning_rate': 1.4245014245014247e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999439461883408, 'eval_recall': 0.9988795518207283, 'eval_f1': 0.9991594284113197, 'eval_loss': 0.004332180134952068, 'eval_runtime': 31.5281, 'eval_samples_per_second': 356.032, 'eval_steps_per_second': 22.266, 'epoch': 3.0}
{'train_runtime': 417.0355, 'train_samples_per_second': 80.741, 'train_steps_per_second': 5.05, 'train_loss': 0.02729550994143725, 'epoch': 3.0}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999439461883408, 'eval_recall': 0.9988795518207283, 'eval_f1': 0.9991594284113197, 'eval_loss': 0.004332180134952068, 'eval_runtime': 31.4917, 'eval_samples_per_second': 356.443, 'eval_steps_per_second': 22.292, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.4419, 'grad_norm': 3.4116175174713135, 'learning_rate': 4.76258309591643e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0367, 'grad_norm': 0.20234414935112, 'learning_rate': 4.525166191832859e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0185, 'grad_norm': 10.53183650970459, 'learning_rate': 4.287749287749288e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0098, 'grad_norm': 0.06788452714681625, 'learning_rate': 4.050332383665717e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0116, 'grad_norm': 0.05349056050181389, 'learning_rate': 3.8129154795821466e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0173, 'grad_norm': 0.06665904074907303, 'learning_rate': 3.5754985754985762e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0047, 'grad_norm': 0.034641653299331665, 'learning_rate': 3.338081671415005e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9988417676407698, 'eval_precision': 0.9986928104575163, 'eval_recall': 0.9988793425476279, 'eval_f1': 0.9987860677934448, 'eval_loss': 0.00623658811673522, 'eval_runtime': 31.4276, 'eval_samples_per_second': 357.138, 'eval_steps_per_second': 22.337, 'epoch': 1.0}
{'loss': 0.0104, 'grad_norm': 2.0170187950134277, 'learning_rate': 3.1006647673314343e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0018, 'grad_norm': 0.027970565482974052, 'learning_rate': 2.8632478632478635e-06, 'epoch': 1.282051282051282}
{'loss': 0.0017, 'grad_norm': 0.02395373024046421, 'learning_rate': 2.6258309591642927e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0051, 'grad_norm': 0.02434173971414566, 'learning_rate': 2.388414055080722e-06, 'epoch': 1.566951566951567}
{'loss': 0.0053, 'grad_norm': 0.02077903226017952, 'learning_rate': 2.150997150997151e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0015, 'grad_norm': 0.0180644728243351, 'learning_rate': 1.9135802469135804e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0097, 'grad_norm': 0.03590639680624008, 'learning_rate': 1.6761633428300099e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.999376336421953, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.999252895031752, 'eval_f1': 0.9993462220976931, 'eval_loss': 0.00406847195699811, 'eval_runtime': 31.5718, 'eval_samples_per_second': 355.507, 'eval_steps_per_second': 22.235, 'epoch': 2.0}
{'loss': 0.0034, 'grad_norm': 0.014726060442626476, 'learning_rate': 1.4387464387464389e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0009, 'grad_norm': 0.01783304288983345, 'learning_rate': 1.2013295346628681e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0039, 'grad_norm': 0.013789552263915539, 'learning_rate': 9.639126305792973e-07, 'epoch': 2.421652421652422}
{'loss': 0.0044, 'grad_norm': 0.016324292868375778, 'learning_rate': 7.264957264957266e-07, 'epoch': 2.564102564102564}
{'loss': 0.0053, 'grad_norm': 0.015814635902643204, 'learning_rate': 4.890788224121558e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0053, 'grad_norm': 0.015851201489567757, 'learning_rate': 2.51661918328585e-07, 'epoch': 2.849002849002849}
{'loss': 0.0008, 'grad_norm': 0.013362285681068897, 'learning_rate': 1.4245014245014247e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9994654312188168, 'eval_precision': 0.999439671273814, 'eval_recall': 0.999439671273814, 'eval_f1': 0.999439671273814, 'eval_loss': 0.003873038338497281, 'eval_runtime': 31.5232, 'eval_samples_per_second': 356.055, 'eval_steps_per_second': 22.269, 'epoch': 3.0}
{'train_runtime': 407.5996, 'train_samples_per_second': 82.618, 'train_steps_per_second': 5.167, 'train_loss': 0.028492179562152848, 'epoch': 3.0}
{'eval_accuracy': 0.9994654312188168, 'eval_precision': 0.999439671273814, 'eval_recall': 0.999439671273814, 'eval_f1': 0.999439671273814, 'eval_loss': 0.003873038338497281, 'eval_runtime': 31.405, 'eval_samples_per_second': 357.396, 'eval_steps_per_second': 22.353, 'epoch': 3.0}
LR 5.0e-06 - Media Inner Acc: 0.9993
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2701, 'grad_norm': 0.20913198590278625, 'learning_rate': 9.52516619183286e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0131, 'grad_norm': 0.8265618085861206, 'learning_rate': 9.050332383665718e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0147, 'grad_norm': 0.03988269343972206, 'learning_rate': 8.575498575498576e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0104, 'grad_norm': 0.03138205036520958, 'learning_rate': 8.100664767331435e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0028, 'grad_norm': 0.017642807215452194, 'learning_rate': 7.625830959164293e-06, 'epoch': 0.7122507122507122}
{'loss': 0.004, 'grad_norm': 0.015543975867331028, 'learning_rate': 7.1509971509971524e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0012, 'grad_norm': 0.010943196713924408, 'learning_rate': 6.67616334283001e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999439461883408, 'eval_recall': 0.9988795518207283, 'eval_f1': 0.9991594284113197, 'eval_loss': 0.004971285350620747, 'eval_runtime': 31.5265, 'eval_samples_per_second': 356.05, 'eval_steps_per_second': 22.267, 'epoch': 1.0}
{'loss': 0.0041, 'grad_norm': 0.011287099681794643, 'learning_rate': 6.2013295346628685e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0083, 'grad_norm': 0.009265950880944729, 'learning_rate': 5.726495726495727e-06, 'epoch': 1.282051282051282}
{'loss': 0.0011, 'grad_norm': 0.008993229828774929, 'learning_rate': 5.2516619183285854e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0004, 'grad_norm': 0.007661253679543734, 'learning_rate': 4.776828110161444e-06, 'epoch': 1.566951566951567}
{'loss': 0.0004, 'grad_norm': 0.006959748454391956, 'learning_rate': 4.301994301994302e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0033, 'grad_norm': 0.005362387280911207, 'learning_rate': 3.827160493827161e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0054, 'grad_norm': 0.00651908153668046, 'learning_rate': 3.3523266856600197e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9992873051224944, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.999252895031752, 'eval_loss': 0.004094782751053572, 'eval_runtime': 31.5256, 'eval_samples_per_second': 356.06, 'eval_steps_per_second': 22.268, 'epoch': 2.0}
{'loss': 0.0003, 'grad_norm': 0.0053613740019500256, 'learning_rate': 2.8774928774928778e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0053, 'grad_norm': 0.007922796532511711, 'learning_rate': 2.4026590693257362e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0003, 'grad_norm': 0.005865833256393671, 'learning_rate': 1.9278252611585947e-06, 'epoch': 2.421652421652422}
{'loss': 0.0003, 'grad_norm': 0.0062926881946623325, 'learning_rate': 1.4529914529914531e-06, 'epoch': 2.564102564102564}
{'loss': 0.0003, 'grad_norm': 0.004856482148170471, 'learning_rate': 9.781576448243116e-07, 'epoch': 2.7065527065527064}
{'loss': 0.0049, 'grad_norm': 0.005957853980362415, 'learning_rate': 5.0332383665717e-07, 'epoch': 2.849002849002849}
{'loss': 0.0003, 'grad_norm': 0.0047797225415706635, 'learning_rate': 2.8490028490028493e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9992873051224944, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.999252895031752, 'eval_loss': 0.00408810842782259, 'eval_runtime': 31.4346, 'eval_samples_per_second': 357.091, 'eval_steps_per_second': 22.332, 'epoch': 3.0}
{'train_runtime': 406.7997, 'train_samples_per_second': 82.773, 'train_steps_per_second': 5.177, 'train_loss': 0.01666610301932159, 'epoch': 3.0}
{'eval_accuracy': 0.9992873051224944, 'eval_precision': 0.9994395665981692, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.999252895031752, 'eval_loss': 0.00408810842782259, 'eval_runtime': 31.4447, 'eval_samples_per_second': 356.976, 'eval_steps_per_second': 22.325, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2707, 'grad_norm': 8.869606971740723, 'learning_rate': 9.52516619183286e-06, 'epoch': 0.14245014245014245}
{'loss': 0.0094, 'grad_norm': 0.0670756995677948, 'learning_rate': 9.050332383665718e-06, 'epoch': 0.2849002849002849}
{'loss': 0.0113, 'grad_norm': 3.8986997604370117, 'learning_rate': 8.575498575498576e-06, 'epoch': 0.42735042735042733}
{'loss': 0.0052, 'grad_norm': 0.03514880686998367, 'learning_rate': 8.100664767331435e-06, 'epoch': 0.5698005698005698}
{'loss': 0.0102, 'grad_norm': 0.02316158451139927, 'learning_rate': 7.625830959164293e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0135, 'grad_norm': 0.04228588566184044, 'learning_rate': 7.1509971509971524e-06, 'epoch': 0.8547008547008547}
{'loss': 0.0013, 'grad_norm': 0.013826875016093254, 'learning_rate': 6.67616334283001e-06, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9995545260156807, 'eval_precision': 0.9996263777321128, 'eval_recall': 0.999439671273814, 'eval_f1': 0.9995330157840665, 'eval_loss': 0.0035168095491826534, 'eval_runtime': 31.4243, 'eval_samples_per_second': 357.176, 'eval_steps_per_second': 22.339, 'epoch': 1.0}
{'loss': 0.0028, 'grad_norm': 0.045656513422727585, 'learning_rate': 6.2013295346628685e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0006, 'grad_norm': 0.010665983892977238, 'learning_rate': 5.726495726495727e-06, 'epoch': 1.282051282051282}
{'loss': 0.0005, 'grad_norm': 0.008609587326645851, 'learning_rate': 5.2516619183285854e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0036, 'grad_norm': 0.010571889579296112, 'learning_rate': 4.776828110161444e-06, 'epoch': 1.566951566951567}
{'loss': 0.0047, 'grad_norm': 0.011747525073587894, 'learning_rate': 4.301994301994302e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0004, 'grad_norm': 0.007148738484829664, 'learning_rate': 3.827160493827161e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0084, 'grad_norm': 0.02096857689321041, 'learning_rate': 3.3523266856600197e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9992872416250891, 'eval_precision': 0.9988801791713325, 'eval_recall': 0.9996264475158759, 'eval_f1': 0.9992531740104555, 'eval_loss': 0.003520032623782754, 'eval_runtime': 31.4494, 'eval_samples_per_second': 356.891, 'eval_steps_per_second': 22.322, 'epoch': 2.0}
{'train_runtime': 270.4242, 'train_samples_per_second': 124.527, 'train_steps_per_second': 7.788, 'train_loss': 0.024403789735391226, 'epoch': 2.0}
{'eval_accuracy': 0.9995545260156807, 'eval_precision': 0.9996263777321128, 'eval_recall': 0.999439671273814, 'eval_f1': 0.9995330157840665, 'eval_loss': 0.0035168095491826534, 'eval_runtime': 31.4342, 'eval_samples_per_second': 357.063, 'eval_steps_per_second': 22.332, 'epoch': 2.0}
LR 1.0e-05 - Media Inner Acc: 0.9994
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2105, 'grad_norm': 0.2782462537288666, 'learning_rate': 1.4287749287749289e-05, 'epoch': 0.14245014245014245}
{'loss': 0.0088, 'grad_norm': 1.027717113494873, 'learning_rate': 1.3575498575498576e-05, 'epoch': 0.2849002849002849}
{'loss': 0.0172, 'grad_norm': 0.031734172254800797, 'learning_rate': 1.2863247863247864e-05, 'epoch': 0.42735042735042733}
{'loss': 0.0099, 'grad_norm': 0.01993468403816223, 'learning_rate': 1.2150997150997151e-05, 'epoch': 0.5698005698005698}
{'loss': 0.0038, 'grad_norm': 0.014997611753642559, 'learning_rate': 1.143874643874644e-05, 'epoch': 0.7122507122507122}
{'loss': 0.0096, 'grad_norm': 0.011731504462659359, 'learning_rate': 1.0726495726495728e-05, 'epoch': 0.8547008547008547}
{'loss': 0.0045, 'grad_norm': 0.009561345912516117, 'learning_rate': 1.0014245014245015e-05, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9986636971046771, 'eval_precision': 0.9992520568436799, 'eval_recall': 0.9979458450046685, 'eval_f1': 0.9985985237783799, 'eval_loss': 0.009509642608463764, 'eval_runtime': 30.5847, 'eval_samples_per_second': 367.013, 'eval_steps_per_second': 22.953, 'epoch': 1.0}
{'loss': 0.0049, 'grad_norm': 0.008803432807326317, 'learning_rate': 9.301994301994302e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0055, 'grad_norm': 0.006448038853704929, 'learning_rate': 8.589743589743589e-06, 'epoch': 1.282051282051282}
{'loss': 0.0046, 'grad_norm': 0.006250189617276192, 'learning_rate': 7.877492877492877e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0003, 'grad_norm': 0.005392543040215969, 'learning_rate': 7.165242165242165e-06, 'epoch': 1.566951566951567}
{'loss': 0.001, 'grad_norm': 0.005179775413125753, 'learning_rate': 6.452991452991453e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0002, 'grad_norm': 0.003809012472629547, 'learning_rate': 5.74074074074074e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0051, 'grad_norm': 0.004804377444088459, 'learning_rate': 5.028490028490029e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999252895031752, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.9991595853954618, 'eval_loss': 0.005714721512049437, 'eval_runtime': 31.0225, 'eval_samples_per_second': 361.835, 'eval_steps_per_second': 22.629, 'epoch': 2.0}
{'loss': 0.0002, 'grad_norm': 0.003951415419578552, 'learning_rate': 4.316239316239317e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0048, 'grad_norm': 3.977921724319458, 'learning_rate': 3.6039886039886043e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0037, 'grad_norm': 0.0036417723167687654, 'learning_rate': 2.8917378917378916e-06, 'epoch': 2.421652421652422}
{'loss': 0.0002, 'grad_norm': 0.004223658703267574, 'learning_rate': 2.1794871794871797e-06, 'epoch': 2.564102564102564}
{'loss': 0.0002, 'grad_norm': 0.003203503554686904, 'learning_rate': 1.4672364672364672e-06, 'epoch': 2.7065527065527064}
{'loss': 0.0047, 'grad_norm': 0.003376920009031892, 'learning_rate': 7.54985754985755e-07, 'epoch': 2.849002849002849}
{'loss': 0.0002, 'grad_norm': 0.002581121167168021, 'learning_rate': 4.2735042735042736e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999252895031752, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.9991595853954618, 'eval_loss': 0.005861371755599976, 'eval_runtime': 31.2821, 'eval_samples_per_second': 358.831, 'eval_steps_per_second': 22.441, 'epoch': 3.0}
{'train_runtime': 2470.8254, 'train_samples_per_second': 13.628, 'train_steps_per_second': 0.852, 'train_loss': 0.01423962717842782, 'epoch': 3.0}
{'eval_accuracy': 0.9991982182628062, 'eval_precision': 0.999252895031752, 'eval_recall': 0.9990662931839402, 'eval_f1': 0.9991595853954618, 'eval_loss': 0.005714721512049437, 'eval_runtime': 31.3465, 'eval_samples_per_second': 358.094, 'eval_steps_per_second': 22.395, 'epoch': 3.0}
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2021, 'grad_norm': 10.66042709350586, 'learning_rate': 1.4287749287749289e-05, 'epoch': 0.14245014245014245}
{'loss': 0.0051, 'grad_norm': 0.036366455256938934, 'learning_rate': 1.3575498575498576e-05, 'epoch': 0.2849002849002849}
{'loss': 0.0088, 'grad_norm': 0.5578882098197937, 'learning_rate': 1.2863247863247864e-05, 'epoch': 0.42735042735042733}
{'loss': 0.0034, 'grad_norm': 0.014027436263859272, 'learning_rate': 1.2150997150997151e-05, 'epoch': 0.5698005698005698}
{'loss': 0.0117, 'grad_norm': 0.012021224945783615, 'learning_rate': 1.143874643874644e-05, 'epoch': 0.7122507122507122}
{'loss': 0.0162, 'grad_norm': 0.03403550013899803, 'learning_rate': 1.0726495726495728e-05, 'epoch': 0.8547008547008547}
{'loss': 0.0007, 'grad_norm': 0.008985938504338264, 'learning_rate': 1.0014245014245015e-05, 'epoch': 0.9971509971509972}
{'eval_accuracy': 0.9995545260156807, 'eval_precision': 0.9996263777321128, 'eval_recall': 0.999439671273814, 'eval_f1': 0.9995330157840665, 'eval_loss': 0.003122609807178378, 'eval_runtime': 30.9385, 'eval_samples_per_second': 362.785, 'eval_steps_per_second': 22.69, 'epoch': 1.0}
{'loss': 0.0005, 'grad_norm': 0.013279614970088005, 'learning_rate': 9.301994301994302e-06, 'epoch': 1.1396011396011396}
{'loss': 0.0003, 'grad_norm': 0.006077414844185114, 'learning_rate': 8.589743589743589e-06, 'epoch': 1.282051282051282}
{'loss': 0.0003, 'grad_norm': 0.004705473314970732, 'learning_rate': 7.877492877492877e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0015, 'grad_norm': 0.006720376200973988, 'learning_rate': 7.165242165242165e-06, 'epoch': 1.566951566951567}
{'loss': 0.0038, 'grad_norm': 0.0060179452411830425, 'learning_rate': 6.452991452991453e-06, 'epoch': 1.7094017094017095}
{'loss': 0.0002, 'grad_norm': 0.003696564119309187, 'learning_rate': 5.74074074074074e-06, 'epoch': 1.8518518518518519}
{'loss': 0.0081, 'grad_norm': 0.006948012858629227, 'learning_rate': 5.028490028490029e-06, 'epoch': 1.9943019943019942}
{'eval_accuracy': 0.9994654312188168, 'eval_precision': 0.999439671273814, 'eval_recall': 0.999439671273814, 'eval_f1': 0.999439671273814, 'eval_loss': 0.0030504234600812197, 'eval_runtime': 30.7487, 'eval_samples_per_second': 365.023, 'eval_steps_per_second': 22.83, 'epoch': 2.0}
{'loss': 0.0002, 'grad_norm': 0.003184189321473241, 'learning_rate': 4.316239316239317e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0002, 'grad_norm': 0.003069901140406728, 'learning_rate': 3.6039886039886043e-06, 'epoch': 2.2792022792022792}
{'loss': 0.0002, 'grad_norm': 0.0031336701940745115, 'learning_rate': 2.8917378917378916e-06, 'epoch': 2.421652421652422}
{'loss': 0.0003, 'grad_norm': 0.0032993562053889036, 'learning_rate': 2.1794871794871797e-06, 'epoch': 2.564102564102564}
{'loss': 0.0027, 'grad_norm': 0.0031200542580336332, 'learning_rate': 1.4672364672364672e-06, 'epoch': 2.7065527065527064}
{'loss': 0.0002, 'grad_norm': 0.00384593871422112, 'learning_rate': 7.54985754985755e-07, 'epoch': 2.849002849002849}
{'loss': 0.0002, 'grad_norm': 0.00298130651935935, 'learning_rate': 4.2735042735042736e-08, 'epoch': 2.9914529914529915}
{'eval_accuracy': 0.9995545260156807, 'eval_precision': 0.9994397759103641, 'eval_recall': 0.9996264475158759, 'eval_f1': 0.9995331029974788, 'eval_loss': 0.003328158287331462, 'eval_runtime': 30.6392, 'eval_samples_per_second': 366.328, 'eval_steps_per_second': 22.912, 'epoch': 3.0}
{'train_runtime': 400.4669, 'train_samples_per_second': 84.089, 'train_steps_per_second': 5.259, 'train_loss': 0.012654679426323076, 'epoch': 3.0}
{'eval_accuracy': 0.9994654312188168, 'eval_precision': 0.999439671273814, 'eval_recall': 0.999439671273814, 'eval_f1': 0.999439671273814, 'eval_loss': 0.0030504234600812197, 'eval_runtime': 30.6914, 'eval_samples_per_second': 365.705, 'eval_steps_per_second': 22.873, 'epoch': 3.0}
LR 1.5e-05 - Media Inner Acc: 0.9993
Miglior LR per Fold 2: 1e-05 (Acc: 0.9994)
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.0624, 'grad_norm': 0.020218264311552048, 'learning_rate': 8.812915479582148e-06, 'epoch': 0.3561253561253561}
{'loss': 0.0019, 'grad_norm': 0.007785971742123365, 'learning_rate': 7.625830959164293e-06, 'epoch': 0.7122507122507122}
{'loss': 0.0037, 'grad_norm': 0.0037603462114930153, 'learning_rate': 6.438746438746439e-06, 'epoch': 1.0683760683760684}
{'loss': 0.0024, 'grad_norm': 0.002995686139911413, 'learning_rate': 5.2516619183285854e-06, 'epoch': 1.4245014245014245}
{'loss': 0.0013, 'grad_norm': 0.0026965278666466475, 'learning_rate': 4.064577397910732e-06, 'epoch': 1.7806267806267806}
{'loss': 0.0019, 'grad_norm': 0.0023501855321228504, 'learning_rate': 2.8774928774928778e-06, 'epoch': 2.1367521367521367}
{'loss': 0.0021, 'grad_norm': 0.001844245009124279, 'learning_rate': 1.6904083570750237e-06, 'epoch': 2.492877492877493}
{'loss': 0.0003, 'grad_norm': 0.001556280069053173, 'learning_rate': 5.0332383665717e-07, 'epoch': 2.849002849002849}
{'train_runtime': 605.666, 'train_samples_per_second': 111.195, 'train_steps_per_second': 6.954, 'train_loss': 0.009022106841057963, 'epoch': 3.0}
Accuracy TEST Fold 2: 
Risultati Nested Cross-Validation 
Accuratezze Nested CV: [0.9997 0.9993]
Media Nested CV: 0.9995 Â± 0.0002

Test t (baseline = 0.5): t=2242.7000, p=0.000284
Risultato: Differenza statisticamente significativa.
Addestramento finale sull'intero dataset (Train/Val Split)
Miglior Learning Rate (Media Nested CV): 1.0e-05
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\aletu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'loss': 0.2707, 'grad_norm': 0.2130819410085678, 'learning_rate': 9.89821882951654e-06, 'epoch': 0.05089058524173028}
{'loss': 0.0127, 'grad_norm': 0.07432493567466736, 'learning_rate': 9.79643765903308e-06, 'epoch': 0.10178117048346055}
{'loss': 0.0059, 'grad_norm': 0.0323222316801548, 'learning_rate': 9.694656488549619e-06, 'epoch': 0.15267175572519084}
{'loss': 0.0101, 'grad_norm': 0.028667792677879333, 'learning_rate': 9.592875318066158e-06, 'epoch': 0.2035623409669211}
{'loss': 0.0108, 'grad_norm': 0.020124418660998344, 'learning_rate': 9.491094147582697e-06, 'epoch': 0.2544529262086514}
{'loss': 0.0099, 'grad_norm': 0.014901798218488693, 'learning_rate': 9.389312977099237e-06, 'epoch': 0.3053435114503817}
{'loss': 0.0082, 'grad_norm': 0.017390279099345207, 'learning_rate': 9.287531806615778e-06, 'epoch': 0.356234096692112}
{'loss': 0.0006, 'grad_norm': 0.010569505393505096, 'learning_rate': 9.185750636132317e-06, 'epoch': 0.4071246819338422}
{'loss': 0.0005, 'grad_norm': 0.01454093586653471, 'learning_rate': 9.083969465648855e-06, 'epoch': 0.4580152671755725}
{'loss': 0.0036, 'grad_norm': 0.006900561973452568, 'learning_rate': 8.982188295165396e-06, 'epoch': 0.5089058524173028}
{'loss': 0.0003, 'grad_norm': 0.004407336935400963, 'learning_rate': 8.880407124681935e-06, 'epoch': 0.5597964376590331}
{'loss': 0.0004, 'grad_norm': 0.003958348650485277, 'learning_rate': 8.778625954198474e-06, 'epoch': 0.6106870229007634}
{'loss': 0.0054, 'grad_norm': 0.006578837987035513, 'learning_rate': 8.676844783715014e-06, 'epoch': 0.6615776081424937}
{'loss': 0.004, 'grad_norm': 0.004498537164181471, 'learning_rate': 8.575063613231553e-06, 'epoch': 0.712468193384224}
{'loss': 0.0057, 'grad_norm': 0.0036221854388713837, 'learning_rate': 8.473282442748092e-06, 'epoch': 0.7633587786259542}
{'loss': 0.0057, 'grad_norm': 0.003950980491936207, 'learning_rate': 8.371501272264631e-06, 'epoch': 0.8142493638676844}
{'loss': 0.0048, 'grad_norm': 0.005624891258776188, 'learning_rate': 8.26972010178117e-06, 'epoch': 0.8651399491094147}
{'loss': 0.005, 'grad_norm': 0.006683390587568283, 'learning_rate': 8.16793893129771e-06, 'epoch': 0.916030534351145}
{'loss': 0.0047, 'grad_norm': 0.004444936756044626, 'learning_rate': 8.066157760814251e-06, 'epoch': 0.9669211195928753}
{'eval_accuracy': 0.9991091314031181, 'eval_precision': 0.998755832037325, 'eval_recall': 0.9993775287892935, 'eval_f1': 0.9990665836963286, 'eval_loss': 0.0033257470931857824, 'eval_runtime': 18.7065, 'eval_samples_per_second': 360.036, 'eval_steps_per_second': 22.506, 'epoch': 1.0}
{'loss': 0.0024, 'grad_norm': 0.004104748368263245, 'learning_rate': 7.964376590330789e-06, 'epoch': 1.0178117048346056}
{'loss': 0.0001, 'grad_norm': 0.0027868098113685846, 'learning_rate': 7.862595419847328e-06, 'epoch': 1.0687022900763359}
{'loss': 0.0093, 'grad_norm': 0.019664963707327843, 'learning_rate': 7.760814249363869e-06, 'epoch': 1.1195928753180662}
{'loss': 0.0043, 'grad_norm': 0.00338949472643435, 'learning_rate': 7.659033078880408e-06, 'epoch': 1.1704834605597965}
{'loss': 0.0002, 'grad_norm': 0.0020703121554106474, 'learning_rate': 7.557251908396948e-06, 'epoch': 1.2213740458015268}
{'loss': 0.0001, 'grad_norm': 0.0023246442433446646, 'learning_rate': 7.455470737913486e-06, 'epoch': 1.272264631043257}
{'loss': 0.0028, 'grad_norm': 0.003388927085325122, 'learning_rate': 7.3536895674300254e-06, 'epoch': 1.3231552162849873}
{'loss': 0.0028, 'grad_norm': 0.001980855595320463, 'learning_rate': 7.251908396946566e-06, 'epoch': 1.3740458015267176}
{'loss': 0.0046, 'grad_norm': 0.010002506896853447, 'learning_rate': 7.150127226463105e-06, 'epoch': 1.424936386768448}
{'loss': 0.0104, 'grad_norm': 0.016432520002126694, 'learning_rate': 7.048346055979644e-06, 'epoch': 1.4758269720101782}
{'loss': 0.0042, 'grad_norm': 0.2558959126472473, 'learning_rate': 6.946564885496184e-06, 'epoch': 1.5267175572519083}
{'loss': 0.0039, 'grad_norm': 0.007996260188519955, 'learning_rate': 6.844783715012723e-06, 'epoch': 1.5776081424936388}
{'loss': 0.0002, 'grad_norm': 0.0024626879021525383, 'learning_rate': 6.743002544529262e-06, 'epoch': 1.6284987277353689}
{'loss': 0.0002, 'grad_norm': 0.0011400253279134631, 'learning_rate': 6.641221374045802e-06, 'epoch': 1.6793893129770994}
{'loss': 0.001, 'grad_norm': 0.0013112163869664073, 'learning_rate': 6.539440203562342e-06, 'epoch': 1.7302798982188294}
{'loss': 0.0083, 'grad_norm': 0.0026737810112535954, 'learning_rate': 6.437659033078881e-06, 'epoch': 1.78117048346056}
{'loss': 0.0001, 'grad_norm': 0.001616195891983807, 'learning_rate': 6.335877862595419e-06, 'epoch': 1.83206106870229}
{'loss': 0.0001, 'grad_norm': 0.0013066873652860522, 'learning_rate': 6.2340966921119596e-06, 'epoch': 1.8829516539440203}
{'loss': 0.0001, 'grad_norm': 0.002016759477555752, 'learning_rate': 6.132315521628499e-06, 'epoch': 1.9338422391857506}
{'loss': 0.0001, 'grad_norm': 0.0015849326737225056, 'learning_rate': 6.030534351145039e-06, 'epoch': 1.984732824427481}
{'eval_accuracy': 0.9988121752041574, 'eval_precision': 0.9981349082996581, 'eval_recall': 0.9993775287892935, 'eval_f1': 0.998755832037325, 'eval_loss': 0.003926095087081194, 'eval_runtime': 18.7306, 'eval_samples_per_second': 359.572, 'eval_steps_per_second': 22.477, 'epoch': 2.0}
{'loss': 0.0001, 'grad_norm': 0.0017396487528458238, 'learning_rate': 5.928753180661578e-06, 'epoch': 2.035623409669211}
{'loss': 0.0062, 'grad_norm': 0.01497296616435051, 'learning_rate': 5.826972010178118e-06, 'epoch': 2.0865139949109412}
{'loss': 0.0001, 'grad_norm': 0.001560130505822599, 'learning_rate': 5.725190839694656e-06, 'epoch': 2.1374045801526718}
{'loss': 0.0001, 'grad_norm': 0.0007975390763022006, 'learning_rate': 5.623409669211196e-06, 'epoch': 2.188295165394402}
{'loss': 0.0014, 'grad_norm': 0.0016431676922366023, 'learning_rate': 5.521628498727736e-06, 'epoch': 2.2391857506361323}
{'loss': 0.0001, 'grad_norm': 0.0013485633535310626, 'learning_rate': 5.419847328244276e-06, 'epoch': 2.2900763358778624}
{'loss': 0.0, 'grad_norm': 0.0012712012976408005, 'learning_rate': 5.318066157760815e-06, 'epoch': 2.340966921119593}
{'loss': 0.0, 'grad_norm': 0.0009589236578904092, 'learning_rate': 5.216284987277354e-06, 'epoch': 2.391857506361323}
{'loss': 0.0008, 'grad_norm': 0.0007083105156198144, 'learning_rate': 5.114503816793893e-06, 'epoch': 2.4427480916030535}
{'loss': 0.0, 'grad_norm': 0.0005127894110046327, 'learning_rate': 5.012722646310433e-06, 'epoch': 2.4936386768447836}
{'loss': 0.0, 'grad_norm': 0.0004119888471905142, 'learning_rate': 4.910941475826972e-06, 'epoch': 2.544529262086514}
{'loss': 0.0, 'grad_norm': 0.0005529249319806695, 'learning_rate': 4.8091603053435125e-06, 'epoch': 2.595419847328244}
{'loss': 0.0, 'grad_norm': 0.000618154474068433, 'learning_rate': 4.707379134860051e-06, 'epoch': 2.6463104325699747}
{'loss': 0.0, 'grad_norm': 0.0004917463520541787, 'learning_rate': 4.605597964376591e-06, 'epoch': 2.6972010178117047}
{'loss': 0.0, 'grad_norm': 0.0004394478746689856, 'learning_rate': 4.5038167938931296e-06, 'epoch': 2.7480916030534353}
{'loss': 0.0, 'grad_norm': 0.0003646125551313162, 'learning_rate': 4.40203562340967e-06, 'epoch': 2.7989821882951653}
{'loss': 0.0, 'grad_norm': 0.00033510092180222273, 'learning_rate': 4.300254452926209e-06, 'epoch': 2.849872773536896}
{'loss': 0.0, 'grad_norm': 0.0005682865739800036, 'learning_rate': 4.198473282442748e-06, 'epoch': 2.900763358778626}
{'loss': 0.0, 'grad_norm': 0.00040593548328615725, 'learning_rate': 4.096692111959288e-06, 'epoch': 2.9516539440203564}
{'eval_accuracy': 0.9997030438010394, 'eval_precision': 1.0, 'eval_recall': 0.9993775287892935, 'eval_f1': 0.9996886674968867, 'eval_loss': 0.0026565163861960173, 'eval_runtime': 18.7315, 'eval_samples_per_second': 359.555, 'eval_steps_per_second': 22.475, 'epoch': 3.0}
{'loss': 0.0, 'grad_norm': 0.00048481341218575835, 'learning_rate': 3.994910941475828e-06, 'epoch': 3.0025445292620865}
{'loss': 0.0, 'grad_norm': 0.0003742374829016626, 'learning_rate': 3.893129770992366e-06, 'epoch': 3.053435114503817}
{'loss': 0.0, 'grad_norm': 0.0002730338310357183, 'learning_rate': 3.791348600508906e-06, 'epoch': 3.104325699745547}
{'loss': 0.0, 'grad_norm': 0.00038597380626015365, 'learning_rate': 3.6895674300254457e-06, 'epoch': 3.1552162849872776}
{'loss': 0.0, 'grad_norm': 0.0004394669085741043, 'learning_rate': 3.587786259541985e-06, 'epoch': 3.2061068702290076}
{'loss': 0.0, 'grad_norm': 0.0003311442560516298, 'learning_rate': 3.4860050890585244e-06, 'epoch': 3.2569974554707377}
{'loss': 0.0, 'grad_norm': 0.0002344035019632429, 'learning_rate': 3.384223918575064e-06, 'epoch': 3.3078880407124682}
{'loss': 0.0, 'grad_norm': 0.00036430114414542913, 'learning_rate': 3.2824427480916034e-06, 'epoch': 3.3587786259541983}
{'loss': 0.0, 'grad_norm': 0.0001995716302189976, 'learning_rate': 3.1806615776081427e-06, 'epoch': 3.409669211195929}
{'loss': 0.0, 'grad_norm': 0.00027716479962691665, 'learning_rate': 3.078880407124682e-06, 'epoch': 3.460559796437659}
{'loss': 0.0, 'grad_norm': 0.000223042065044865, 'learning_rate': 2.9770992366412218e-06, 'epoch': 3.5114503816793894}
{'loss': 0.0, 'grad_norm': 0.00016327099001500756, 'learning_rate': 2.875318066157761e-06, 'epoch': 3.5623409669211195}
{'loss': 0.0, 'grad_norm': 0.00018709474534261972, 'learning_rate': 2.7735368956743004e-06, 'epoch': 3.61323155216285}
{'loss': 0.0, 'grad_norm': 0.000206887794774957, 'learning_rate': 2.67175572519084e-06, 'epoch': 3.66412213740458}
{'loss': 0.0, 'grad_norm': 0.00017520315304864198, 'learning_rate': 2.5699745547073794e-06, 'epoch': 3.7150127226463106}
{'loss': 0.0, 'grad_norm': 0.00024882133584469557, 'learning_rate': 2.4681933842239187e-06, 'epoch': 3.7659033078880406}
{'loss': 0.0, 'grad_norm': 0.00023925739515107125, 'learning_rate': 2.3664122137404585e-06, 'epoch': 3.816793893129771}
{'loss': 0.0, 'grad_norm': 0.00023946957662701607, 'learning_rate': 2.2646310432569978e-06, 'epoch': 3.867684478371501}
{'loss': 0.0002, 'grad_norm': 0.00027504359604790807, 'learning_rate': 2.162849872773537e-06, 'epoch': 3.9185750636132317}
{'loss': 0.0, 'grad_norm': 0.00023944469285197556, 'learning_rate': 2.0610687022900764e-06, 'epoch': 3.969465648854962}
{'eval_accuracy': 0.9997030438010394, 'eval_precision': 1.0, 'eval_recall': 0.9993775287892935, 'eval_f1': 0.9996886674968867, 'eval_loss': 0.0026223703753203154, 'eval_runtime': 18.7344, 'eval_samples_per_second': 359.499, 'eval_steps_per_second': 22.472, 'epoch': 4.0}
{'loss': 0.0, 'grad_norm': 0.00022883345081936568, 'learning_rate': 1.959287531806616e-06, 'epoch': 4.020356234096692}
{'loss': 0.0, 'grad_norm': 0.00019381553283892572, 'learning_rate': 1.8575063613231552e-06, 'epoch': 4.071246819338422}
{'loss': 0.0, 'grad_norm': 0.00021527781791519374, 'learning_rate': 1.7557251908396948e-06, 'epoch': 4.122137404580153}
{'loss': 0.0, 'grad_norm': 0.00016021623741835356, 'learning_rate': 1.6539440203562343e-06, 'epoch': 4.1730279898218825}
{'loss': 0.0, 'grad_norm': 0.00016892787243705243, 'learning_rate': 1.5521628498727736e-06, 'epoch': 4.223918575063613}
{'loss': 0.0059, 'grad_norm': 0.00020817623590119183, 'learning_rate': 1.450381679389313e-06, 'epoch': 4.2748091603053435}
{'loss': 0.0, 'grad_norm': 0.00026055111084133387, 'learning_rate': 1.3486005089058526e-06, 'epoch': 4.325699745547074}
{'loss': 0.0, 'grad_norm': 0.00023664518084842712, 'learning_rate': 1.246819338422392e-06, 'epoch': 4.376590330788804}
{'loss': 0.0, 'grad_norm': 0.0002073266514344141, 'learning_rate': 1.1450381679389313e-06, 'epoch': 4.427480916030534}
{'loss': 0.0, 'grad_norm': 0.00032226592884398997, 'learning_rate': 1.0432569974554708e-06, 'epoch': 4.478371501272265}
{'loss': 0.0, 'grad_norm': 0.0001263120793737471, 'learning_rate': 9.414758269720102e-07, 'epoch': 4.529262086513995}
{'loss': 0.0, 'grad_norm': 0.0001565345737617463, 'learning_rate': 8.396946564885497e-07, 'epoch': 4.580152671755725}
{'loss': 0.0, 'grad_norm': 0.00015150364197324961, 'learning_rate': 7.37913486005089e-07, 'epoch': 4.631043256997455}
{'loss': 0.0, 'grad_norm': 0.00014900848327670246, 'learning_rate': 6.361323155216286e-07, 'epoch': 4.681933842239186}
{'loss': 0.0, 'grad_norm': 0.0001333809195784852, 'learning_rate': 5.34351145038168e-07, 'epoch': 4.732824427480916}
{'loss': 0.0, 'grad_norm': 0.00046228402061387897, 'learning_rate': 4.325699745547074e-07, 'epoch': 4.783715012722646}
{'loss': 0.0, 'grad_norm': 0.00017402235243935138, 'learning_rate': 3.307888040712468e-07, 'epoch': 4.8346055979643765}
{'loss': 0.0, 'grad_norm': 0.00036231736885383725, 'learning_rate': 2.2900763358778629e-07, 'epoch': 4.885496183206107}
{'loss': 0.0, 'grad_norm': 0.0002888244634959847, 'learning_rate': 1.272264631043257e-07, 'epoch': 4.9363867684478375}
{'loss': 0.0, 'grad_norm': 0.00014718488091602921, 'learning_rate': 2.5445292620865142e-08, 'epoch': 4.987277353689567}
{'eval_accuracy': 0.9997030438010394, 'eval_precision': 1.0, 'eval_recall': 0.9993775287892935, 'eval_f1': 0.9996886674968867, 'eval_loss': 0.0017935159849002957, 'eval_runtime': 18.7641, 'eval_samples_per_second': 358.929, 'eval_steps_per_second': 22.436, 'epoch': 5.0}
{'train_runtime': 1527.8713, 'train_samples_per_second': 102.849, 'train_steps_per_second': 6.431, 'train_loss': 0.004477953212346074, 'epoch': 5.0}

Modello finale salvato in roberta_finale

Modello finale salvato in distilbert_finale

----------------------------------------
Metriche finali su TEST (Modello migliore):
----------------------------------------
              precision    recall  f1-score   support

    Fake (0)       1.00      1.00      1.00      3522
    True (1)       1.00      1.00      1.00      3213

    accuracy                           1.00      6735
   macro avg       1.00      1.00      1.00      6735
weighted avg       1.00      1.00      1.00      6735

Accuracy:  1.0000
Precision: 1.0000
Recall:    1.0000
F1-score:  1.0000